{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERIFICAR RECURSOS DEL SISTEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"GPUs disponibles:\", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA está disponible. Número de GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Nombre de la GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA no está disponible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "\n",
    "def check_cpus():\n",
    "    print(\"Número de CPUs (os.cpu_count()):\", os.cpu_count())\n",
    "    print(\"Número de CPUs (multiprocessing.cpu_count()):\", multiprocessing.cpu_count())\n",
    "\n",
    "def check_gpu_pytorch():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA está disponible. Número de GPUs:\", torch.cuda.device_count())\n",
    "        print(\"Nombre de la GPU:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print(\"CUDA no está disponible en PyTorch.\")\n",
    "\n",
    "def check_gpu_tensorflow():\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        print(\"GPUs disponibles en TensorFlow:\", len(gpus))\n",
    "        for gpu in gpus:\n",
    "            print(\"Nombre de la GPU:\", gpu.name)\n",
    "    else:\n",
    "        print(\"CUDA no está disponible en TensorFlow.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    check_cpus()\n",
    "    check_gpu_pytorch()\n",
    "    check_gpu_tensorflow()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARGAR Y PREPROCESAR DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta archivo Excel\n",
    "file_path = '/home/jovyan/work/transcripciones_frases_v5.xlsx'\n",
    "\n",
    "# Cargar el archivo Excel\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "#Comprobaciones\n",
    "assert 'Texto Original' in df.columns, \"La columna 'Texto Original' no existe en el archivo Excel\"\n",
    "assert 'Texto Corregido' in df.columns, \"La columna 'Texto Corregido' no existe en el archivo Excel\"\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIVIDIR DATOS DE ENTRENAMIENTO Y VALIDACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CARGAR EL MODELO Y EL TOKENIZADOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# model_name = 't5-base'\n",
    "model_name = (\"vgaraujov/t5-base-spanish\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prefix = \"corregir: \"\n",
    "max_input_length = 300\n",
    "max_target_length = 300\n",
    "\n",
    "# Función de preprocesamiento\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex for ex in examples['Texto Original']]\n",
    "    targets = [ex for ex in examples['Texto Corregido']]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, padding='max_length')\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding='max_length')\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# PReprocesamiento\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=train_dataset.column_names)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=val_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobaciones\n",
    "print(\"Estructura del dataset de entrenamiento después del preprocesamiento:\")\n",
    "print(train_dataset)\n",
    "print(\"Ejemplo de datos preprocesados:\")\n",
    "print(train_dataset[0])\n",
    "\n",
    "print(\"Estructura del dataset de validación después del preprocesamiento:\")\n",
    "print(val_dataset)\n",
    "print(\"Ejemplo de datos preprocesados:\")\n",
    "print(val_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "metric = evaluate.load('sacrebleu')\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\", max_new_tokens=300):\n",
    "        self.model.config.max_length = max_new_tokens\n",
    "        return super().evaluate(eval_dataset=eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
    "\n",
    "\n",
    "def check_data(dataset):\n",
    "    for example in dataset:\n",
    "        input_ids = example.get(\"input_ids\", None)\n",
    "        labels = example.get(\"labels\", None)\n",
    "        if input_ids is None or labels is None:\n",
    "            print(\"Error: 'input_ids' o 'labels' no encontrados en el ejemplo\", example)\n",
    "            continue\n",
    "        if any(id >= tokenizer.vocab_size or id < 0 for id in input_ids):\n",
    "            print(\"Error: input_ids fuera de rango\", input_ids)\n",
    "        if any(id >= tokenizer.vocab_size or (id < 0 and id != -100) for id in labels):\n",
    "            print(\"Error: labels fuera de rango\", labels)\n",
    "\n",
    "# comprobaciones\n",
    "check_data(train_dataset)\n",
    "check_data(val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIGURAR Y ENTRENAR EL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='/home/jovyan/results',  \n",
    "    num_train_epochs=8,               \n",
    "    warmup_steps=100,                    \n",
    "    weight_decay=0.00001,                 #Desintegración del peso\n",
    "    learning_rate=2e-5,\n",
    "    fp16=True,                            #Entrenamiento en punto flotante de 16 bits\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_num_beams=15,              #Número de haces de búsqueda (mayor valor menor invencion de palabras),\n",
    ")\n",
    "\n",
    "trainer = CustomSeq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "#Entrenar\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = '/home/jovyan/work/correct_transcription_model_base_v5'\n",
    "tokenizer_save_path = '/home/jovyan/work/correct_transcription_tokenizer_base_v5'\n",
    "\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(tokenizer_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(preds, labels):\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return result\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels_batch = batch['labels'].to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_target_length)\n",
    "        \n",
    "        preds.extend(outputs.cpu().numpy())\n",
    "        labels.extend(labels_batch.cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(preds, labels)\n",
    "    return metrics\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "#Mover GPU si está disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Evaluar\n",
    "metrics = evaluate_model(model, val_dataloader)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, dataloader, tokenizer, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_target_length, num_beams=4, early_stopping=True)\n",
    "        \n",
    "        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        predictions.extend(preds)\n",
    "        references.extend(refs)\n",
    "    \n",
    "    return predictions, references\n",
    "\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, collate_fn=data_collator)\n",
    "\n",
    "val_predictions, val_references = generate_predictions(model, val_dataloader, tokenizer)\n",
    "\n",
    "references = [[ref] for ref in val_references]\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "bleu_score = metric.compute(predictions=val_predictions, references=references)\n",
    "\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERAR PREDICIONES Y EVALUAR MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_predictions(model, dataloader, tokenizer, max_length=96, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    references = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "        \n",
    "        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        refs = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        predictions.extend(preds)\n",
    "        references.extend(refs)\n",
    "    \n",
    "    return predictions, references\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2, collate_fn=data_collator)\n",
    "val_predictions, val_references = generate_predictions(model, val_dataloader, tokenizer)\n",
    "references = [[ref] for ref in val_references]\n",
    "\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "bleu_score = metric.compute(predictions=val_predictions, references=references)\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model, tokenizer, dataset, max_length=96, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for i in range(len(dataset)):\n",
    "        inputs = tokenizer(dataset[i]['Texto Original'], return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        outputs = model.generate(inputs['input_ids'], max_length=max_length, num_beams=4, early_stopping=True)\n",
    "        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        predictions.append(pred_text)\n",
    "    return predictions\n",
    "\n",
    "val_predictions = generate_predictions(model, tokenizer, val_dataset)\n",
    "references = val_df['Texto Corregido'].tolist()\n",
    "\n",
    "\n",
    "references = [[ref] for ref in references]\n",
    "\n",
    "rouge_score = metric.compute(predictions=val_predictions, references=references)\n",
    "bleu_score = metric.compute(predictions=[pred.split() for pred in val_predictions], references=[[ref.split()] for ref in references])\n",
    "\n",
    "print(\"ROUGE score:\", rouge_score)\n",
    "print(\"BLEU score:\", bleu_score)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
